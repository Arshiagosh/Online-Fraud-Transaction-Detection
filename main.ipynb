{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Fraud Transaction Detection\n",
    "- By **Arshia Goshtasbi**\n",
    "- Github: [@Arshiagosh](https://github.com/Arshiagosh)\n",
    "\n",
    "**Description:**\\\n",
    "This Jupyter Notebook focuses on developing a decision tree algorithm to detect fraudulent online transactions. Fraudulent activities in virtual payments pose significant risks to financial institutions and security systems. Decision trees offer a powerful and interpretable method for classifying data based on feature attributes, making them well-suited for fraud detection tasks.\n",
    "\n",
    "The notebook covers the end-to-end process of building a decision tree model, including data preprocessing, feature engineering, model training, and performance evaluation. Two popular criteria, `entropy` and `Gini index`, are used as splitting criteria for constructing the decision tree.\n",
    "\n",
    "By exploring this notebook, readers can gain insights into the application of decision trees in fraud detection scenarios and understand the differences in performance between the entropy and Gini index as impurity measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "In this section, the necessary libraries are imported, including `NumPy` for numerical computations, `Pandas` for data manipulation, `Seaborn` and `Matplotlib` for data visualization, and `Scikit-learn` for evaluation metrics. Additionally, a custom implementation of the Decision Tree algorithm is imported from the `DT` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from DT import DecisionTree\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The dataset containing online transaction records is loaded from a `CSV` file using Pandas. The `df.info()` function provides an overview of the dataset, including the number of rows and columns, data types, and non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "df=pd.read_csv('onlinefraud.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Data cleaning is a crucial step in any data science project. In this section, the following steps are performed:\n",
    "\n",
    "1) The number of null values in each column is calculated using `df.isnull().sum()`.\n",
    "2) Rows with null values are removed using `df.dropna()`.\n",
    "3) Duplicate rows are removed using `df.drop_duplicates()`.\n",
    "4) The columns `nameOrig` and `nameDest` are dropped as they are deemed not useful for the analysis.\n",
    "5) A quick overview of the cleaned dataset is displayed using `df.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding howmany Nulls are there.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any NaN\n",
    "df = df.dropna()\n",
    "# Removing any Duplicates\n",
    "df = df.drop_duplicates()\n",
    "# Counting the null values in each attribute\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was clean already so that's awesome to know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping useless info\n",
    "df.drop(columns=['nameOrig','nameDest'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Tools\n",
    "This section introduces several preprocessing techniques used in the notebook, including one-hot encoding and binning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding for `type` Column\n",
    "One-hot encoding is a technique used to convert categorical data into a format suitable for machine learning algorithms. It creates binary columns for each category, where a value of `1` indicates the presence of that category, and `0` indicates its absence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for one-hot encoding\n",
    "def one_hot_encode(df):\n",
    "    df_new = df.copy()\n",
    "    # Get the 'type' column\n",
    "    types = df_new['type']\n",
    "    # Get unique types\n",
    "    unique_types = sorted(set(types))\n",
    "    # Create one-hot encoded columns\n",
    "    for t in unique_types:\n",
    "        df_new[t] = (types == t).astype(int)\n",
    "    # Drop the original 'type' column\n",
    "    df_new.drop('type', axis=1, inplace=True)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `one_hot_encode` function takes a DataFrame as input and performs one-hot encoding on the `type` column. It creates new binary columns for each unique value in the `type` column and drops the original `type` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning\n",
    "Binning is a technique used to discretize continuous features or features with a large range of numerical values. It divides the range of values into equal-width intervals (bins) and assigns each value to its corresponding bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_width_binning(df, n_bins=100, column_to_ignore=None):\n",
    "  \"\"\"\n",
    "  Bins each column in the DataFrame using equal-width intervals, excluding the specified column.\n",
    "\n",
    "  Parameters:\n",
    "    df: pandas DataFrame\n",
    "      Input DataFrame to be binned.\n",
    "    n_bins: int, optional (default=100)\n",
    "      Number of bins to create for each column.\n",
    "    column_to_ignore: str or list of str, optional\n",
    "      Column(s) to exclude from binning.\n",
    "\n",
    "  Returns:\n",
    "    df_binned: pandas DataFrame       with each column (except the ignored one) binned using equal-width intervals.\n",
    "  \"\"\"\n",
    "\n",
    "  # Handle potential None value for column_to_ignore\n",
    "  if column_to_ignore is None:\n",
    "    column_to_ignore = []\n",
    "\n",
    "  # Iterate through columns excluding the ones to ignore\n",
    "  for column in df.columns:\n",
    "    if column not in column_to_ignore:\n",
    "      min_val = df[column].min()\n",
    "      max_val = df[column].max()\n",
    "      width = (max_val - min_val) / n_bins\n",
    "      bins = [min_val + i * width for i in range(n_bins + 1)]\n",
    "      df[column] = pd.cut(df[column], bins=bins, labels=False, include_lowest=True, duplicates='drop')\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `equal_width_binning` function takes a `DataFrame`, the number of bins `(n_bins)`, and an optional list of columns to ignore `(column_to_ignore)` as input. It bins each column (except the ignored ones) using equal-width intervals, dividing the range of values into `n_bins` bins. The binned values are then replaced in the original DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating dataset based on `type`\n",
    "The `seperate_on_type` function takes a `DataFrame` and a specific `type` value as input. It creates a new `DataFrame` containing only the rows where the `type` column matches the specified value. Additionally, it drops the `type` column from the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_on_type(df, type_search):\n",
    "    df_new = df.loc[(df.type==type_search)]\n",
    "    df_new = df_new.drop(columns=['type'])\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and test\n",
    "The split_data function is responsible for splitting the dataset into training and testing sets while maintaining a balanced distribution of fraud and non-fraud classes. It takes three arguments:\n",
    "\n",
    "1) `data`: The input DataFrame containing the data.\n",
    "2) `test_size` (default: 0.5): The proportion of data to be included in the test set.\n",
    "3) `pos_ratio` (default: 0.5): The desired proportion of positive (fraud) cases in each set. \\\n",
    "\n",
    "The function first separates the data into fraud and non-fraud instances. It then calculates the sizes of the training and testing sets based on the `test_size` parameter, ensuring integer split sizes to avoid sampling errors. Next, it samples with replacement from the fraud and non-fraud instances to create balanced subsets with the desired `pos_ratio` of positive cases in each set.\n",
    "\n",
    "Finally, the function separates the features `(X_train, X_test)` and labels `(y_train, y_test)` and returns them as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, test_size=0.5, pos_ratio=0.5):\n",
    "  \"\"\"Splits data into training and testing sets with balanced fraud classes and configurable ratios.\n",
    "\n",
    "  Args:\n",
    "    data: A pandas DataFrame containing the data.\n",
    "    test_size: The proportion of data to be included in the test set (default: 0.5).\n",
    "    pos_ratio: The desired proportion of positive (True) class in each set (default: 0.5).\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing four elements: X_train, X_test, y_train, y_test.\n",
    "  \"\"\"\n",
    "\n",
    "  fraud_data = data[data['isFraud'] == True]\n",
    "  non_fraud_data = data[data['isFraud'] == False]\n",
    "\n",
    "  # Ensure data allows for desired positive ratio\n",
    "  min_class_size = min(len(fraud_data), len(non_fraud_data))\n",
    "  min_positive_size = int(min_class_size * pos_ratio)\n",
    "  if min_positive_size > min(len(fraud_data), len(non_fraud_data)):\n",
    "    raise ValueError(\"Data has insufficient positive class to achieve desired ratio\")\n",
    "\n",
    "  # Calculate split sizes based on test_size and desired positive ratio\n",
    "  total_size = len(data)\n",
    "  test_size_adjusted = test_size * total_size\n",
    "  train_size = total_size - test_size_adjusted\n",
    "\n",
    "  # Ensure integer split sizes to avoid sampling errors\n",
    "  positive_train_size = int(train_size * pos_ratio)\n",
    "  negative_train_size = int(train_size - positive_train_size)\n",
    "\n",
    "  positive_test_size = int(test_size_adjusted * pos_ratio)\n",
    "  negative_test_size = int(test_size_adjusted - positive_test_size)\n",
    "\n",
    "  # Sample with replacement to achieve balanced subsets\n",
    "  fraud_train = fraud_data.sample(positive_train_size, replace=True)\n",
    "  if negative_train_size > 0:\n",
    "      non_fraud_train = non_fraud_data.sample(negative_train_size, replace=True)\n",
    "  else:\n",
    "      non_fraud_train = pd.DataFrame()\n",
    "\n",
    "  fraud_test = fraud_data.sample(positive_test_size, replace=True)\n",
    "  if negative_test_size > 0:\n",
    "      non_fraud_test = non_fraud_data.sample(negative_test_size, replace=True)\n",
    "  else:\n",
    "      non_fraud_test = pd.DataFrame()\n",
    "\n",
    "  # Separate features (X) and labels (y)\n",
    "  X_train = pd.concat([fraud_train.drop('isFraud', axis=1), non_fraud_train.drop('isFraud', axis=1)])\n",
    "  y_train = pd.concat([fraud_train['isFraud'], non_fraud_train['isFraud']])\n",
    "\n",
    "  X_test = pd.concat([fraud_test.drop('isFraud', axis=1), non_fraud_test.drop('isFraud', axis=1)])\n",
    "  y_test = pd.concat([fraud_test['isFraud'], non_fraud_test['isFraud']])\n",
    "\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Tools\n",
    "This section introduces two functions for evaluating the performance of the trained models: `print_corr` for visualizing the correlation matrix, and `print_eval` for computing and visualizing various evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "The `print_corr` function takes a `DataFrame` as input and computes the correlation matrix using `df.corr()`. It then creates a heatmap visualization of the correlation matrix using Seaborn's `sns.heatmap`. The heatmap displays the correlation values with annotations, using a diverging colormap ('coolwarm') and a specific format (fmt=\".2f\") for the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_corr(df):\n",
    "    correlation_matrix = df.corr()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Metrics\n",
    "The print_eval function is used to evaluate the performance of a binary classification model. It takes two arguments:\n",
    "\n",
    "1) `y_pred`: The predicted labels from the model.\n",
    "2) `y_test`: The true labels from the test set.\n",
    "The function first computes the confusion matrix using `confusion_matrix(y_test, y_pred)` from Scikit-learn. It then visualizes the confusion matrix using Matplotlib's `plt.imshow`, with annotations for the individual values and a colorbar.\n",
    "\n",
    "Additionally, the function calculates several evaluation metrics:\n",
    "\n",
    "* `Accuracy`: The overall accuracy of the model's predictions.\n",
    "* `Precision`: The ratio of true positives to the sum of true positives and false positives.\n",
    "* `Recall`: The ratio of true positives to the sum of true positives and false negatives.\n",
    "* `F1-Score`: The harmonic mean of precision and recall, providing a balanced measure of the model's performance.\n",
    "\n",
    "These metrics are computed using functions from Scikit-learn and printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval(y_pred, y_test):\n",
    "    # Assuming y_test and y_pred are your test labels and predicted labels, respectively\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.imshow(cm, cmap=plt.cm.Blues, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "    # Add text annotations\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Visualization (Using pydotplus)\n",
    "The `visualize_tree_pydotplus` function takes a `tree` as input and draws the corresponding tree visualization for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz_bin_path = r'C:\\Program Files\\Graphviz\\bin'  # Replace with the actual path on your system\n",
    "os.environ['PATH'] += os.pathsep + graphviz_bin_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree_pydotplus(tree, node_id=0, graph=None):\n",
    "    if not graph:\n",
    "        graph = pydotplus.Dot(graph_type='digraph', rankdir=\"TB\", format='svg')\n",
    "\n",
    "    if tree.value is not None:\n",
    "        label = str(tree.value)\n",
    "    else:\n",
    "        label = f\"X_{tree.feature_index} ? {tree.info_gain:.2f}\"\n",
    "    node = pydotplus.Node(str(node_id), label=label)\n",
    "    graph.add_node(node)\n",
    "\n",
    "    for value, child in tree.children.items():\n",
    "        child_id = node_id * 2 + 1\n",
    "        child_node = pydotplus.Node(str(child_id), label=f\"Value = {value}\")\n",
    "        graph.add_node(child_node)\n",
    "        graph.add_edge(pydotplus.Edge(node, child_node, label=f\"Value = {value}\"))\n",
    "        visualize_tree_pydotplus(child, child_id, graph)\n",
    "        node_id += 1\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "This section presents three different modeling approaches for fraud detection using the decision tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1\n",
    "In this approach:\n",
    "1) The first 2000 rows of the data are used to train the dataset, and the next 2000 rows are used for testing.\n",
    "2) No other preprocessing is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ap1 = df.copy()\n",
    "\n",
    "# Splitting the data for training and testing\n",
    "X_train = df_ap1.drop(columns=['isFraud']).iloc[:2000]\n",
    "y_train = df_ap1['isFraud'].iloc[:2000]\n",
    "\n",
    "X_test = df_ap1.drop(columns=['isFraud']).iloc[2000:4000]\n",
    "y_test = df_ap1['isFraud'].iloc[2000:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='entropy')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='gini')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result\n",
    "This approach takes a significant amount of time, and as seen in the metrics, the model is saturated to 0 and underfits. Some preprocessing methods should be used to improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2\n",
    "In this approach:\n",
    "1) One Hot Encoding is used for the `type` feature to convert the categorical variable into a numerical format.\n",
    "2) Binning is used to transform the continuous features into bins (1000 bins).\n",
    "3) The dataset is split using the split_data function, with two parameters `(test_size, pos_ratio)`:\n",
    "    * `test_size`: The test size ratio.\n",
    "    * `pos_ratio`: The ratio of positive (fraud) samples in both the test set and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oneHot = one_hot_encode(df)\n",
    "df_oneHot = equal_width_binning(df_oneHot, n_bins=1000,\n",
    "                                column_to_ignore=['isFraud','CASH_IN',\n",
    "                                                  'CASH_OUT','DEBIT',\n",
    "                                                  'PAYMENT','TRANSFER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oneHot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(df_oneHot, test_size=0.25, pos_ratio=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='entropy')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='gini')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "The use of One Hot Encoding and Binning techniques improved the model's performance compared to the previous approach.\n",
    "Gini index is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3\n",
    "In this approach:\n",
    "1) Each `type` of transaction is separated into its own dataset.\n",
    "2) Binning is used to transform the continuous features into bins (1000 bins).\n",
    "3) The dataset is split using the `split_data` function, with two parameters (`test_size`, `pos_ratio`):\n",
    "   * `test_size`: The test size ratio.\n",
    "   * `pos_ratio`: The ratio of positive (fraud) samples in both the test set and training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfer = seperate_on_type(df, 'TRANSFER')\n",
    "df_transfer = equal_width_binning(df_transfer, n_bins=1000, column_to_ignore=['isFraud'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_transfer, test_size=0.25, pos_ratio=0.25)\n",
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='entropy')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = visualize_tree_pydotplus(classifier.root)\n",
    "graph.write_png('./Graphs/AP3_TRANSFER_EN.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cash_in = seperate_on_type(df, 'CASH_IN')\n",
    "df_cash_in = equal_width_binning(df_cash_in, n_bins=1000, column_to_ignore=['isFraud'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_cash_in, test_size=0.25, pos_ratio=0.25)\n",
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='entropy')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = visualize_tree_pydotplus(classifier.root)\n",
    "graph.write_png('./Graphs/AP3_CASH_IN_EN.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cash_out = seperate_on_type(df, 'CASH_OUT')\n",
    "df_cash_out = equal_width_binning(df_cash_out, n_bins=1000, column_to_ignore=['isFraud'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_cash_out, test_size=0.25, pos_ratio=0.25)\n",
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='entropy')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = visualize_tree_pydotplus(classifier.root)\n",
    "graph.write_png('./Graphs/AP3_CASH_OUT_EN.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payment = seperate_on_type(df, 'PAYMENT')\n",
    "df_payment = equal_width_binning(df_payment, n_bins=1000, column_to_ignore=['isFraud'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_payment, test_size=0.25, pos_ratio=0.25)\n",
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='entropy')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = visualize_tree_pydotplus(classifier.root)\n",
    "graph.write_png('./Graphs/AP3_PAYMENT_EN.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debit = seperate_on_type(df, 'DEBIT')\n",
    "df_debit = equal_width_binning(df_debit, n_bins=1000, column_to_ignore=['isFraud'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_debit, test_size=0.25, pos_ratio=0.25)\n",
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='entropy')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = visualize_tree_pydotplus(classifier.root)\n",
    "graph.write_png('./Graphs/AP3_DEBIT_EN.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfer = seperate_on_type(df, 'TRANSFER')\n",
    "df_transfer = equal_width_binning(df_transfer, n_bins=1000, column_to_ignore=['isFraud'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_transfer, test_size=0.25, pos_ratio=0.25)\n",
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='gini')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = visualize_tree_pydotplus(classifier.root)\n",
    "graph.write_png('./Graphs/AP3_TRANSFER_GINI.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cash_in = seperate_on_type(df, 'CASH_IN')\n",
    "df_cash_in = equal_width_binning(df_cash_in, n_bins=1000, column_to_ignore=['isFraud'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_cash_in, test_size=0.25, pos_ratio=0.25)\n",
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='gini')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = visualize_tree_pydotplus(classifier.root)\n",
    "graph.write_png('./Graphs/AP3_CASH_IN_GINI.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cash_out = seperate_on_type(df, 'CASH_OUT')\n",
    "df_cash_out = equal_width_binning(df_cash_out, n_bins=1000, column_to_ignore=['isFraud'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_cash_out, test_size=0.25, pos_ratio=0.25)\n",
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='gini')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = visualize_tree_pydotplus(classifier.root)\n",
    "graph.write_png('./Graphs/AP3_CASH_OUT_GINI.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payment = seperate_on_type(df, 'PAYMENT')\n",
    "df_payment = equal_width_binning(df_payment, n_bins=1000, column_to_ignore=['isFraud'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_payment, test_size=0.25, pos_ratio=0.25)\n",
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='gini')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = visualize_tree_pydotplus(classifier.root)\n",
    "graph.write_png('./Graphs/AP3_PAYMENT_GINI.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debit = seperate_on_type(df, 'DEBIT')\n",
    "df_debit = equal_width_binning(df_debit, n_bins=1000, column_to_ignore=['isFraud'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_debit, test_size=0.25, pos_ratio=0.25)\n",
    "classifier = DecisionTree(min_samples_split=2, max_depth=10, mode='gini')\n",
    "classifier.fit(X_train.values,y_train.values.reshape(-1,1))\n",
    "classifier.print_tree()\n",
    "y_pred = classifier.predict(X_test.values)\n",
    "y_pred = [0 if x is None else x for x in y_pred]\n",
    "\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = visualize_tree_pydotplus(classifier.root)\n",
    "graph.write_png('./Graphs/AP3_DEBIT_GINI.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "As seen, separating the data based on the transaction type and training individual models for each type significantly improves the results. For future use, a simple switch case can be employed based on the transaction type, and the suitable model can be selected accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skit-learn and XGBoost Models\n",
    "In this part, models from popular machine learning libraries, such as `Scikit-learn` and `XGBoost`, are used with One Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(df_oneHot, test_size=0.25, pos_ratio=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree -> Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_dt = DecisionTreeClassifier(criterion = 'gini', random_state = 42)\n",
    "classifier_dt.fit(X_train, y_train)\n",
    "y_pred = classifier_dt.predict(X_test)\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree -> Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)\n",
    "classifier_dt.fit(X_train, y_train)\n",
    "y_pred = classifier_dt.predict(X_test)\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_logistic = LogisticRegression(random_state = 42)\n",
    "classifier_logistic.fit(X_train, y_train)\n",
    "y_pred = classifier_logistic.predict(X_test)\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
    "classifier_rf.fit(X_train, y_train)\n",
    "y_pred = classifier_logistic.predict(X_test)\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier_xgb = XGBClassifier()\n",
    "classifier_xgb.fit(X_train, y_train)\n",
    "y_pred = classifier_xgb.predict(X_test)\n",
    "print_eval(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "As seen, even powerful libraries like `Scikit-learn` and `XGBoost` cannot perform well without separating the dataset based on the transaction type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As this Jupyter Notebook demonstrates, in datasets where the overall positive cases are low, the best approach is to separate the dataset into sub-datasets and perform modeling on each subset individually. This technique leads to significantly improved results compared to treating the dataset as a whole.\n",
    "\n",
    "Additionally, the notebook highlights the importance of preprocessing techniques, such as one-hot encoding and binning, in enhancing the performance of machine learning models. By carefully engineering features and handling categorical and continuous variables, the models can better capture the underlying patterns in the data.\n",
    "\n",
    "Overall, this notebook provides a comprehensive exploration of decision tree algorithms for fraud detection in online transactions, emphasizing the importance of data preprocessing, feature engineering, and model evaluation. The insights gained from this analysis can be valuable for practitioners working in the field of fraud detection and risk management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
